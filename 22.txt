22



# Confusion matrix values
true_positive = 90  # cancer = yes predicted correctly
false_positive = 140  # cancer = no predicted as yes
false_negative = 210  # cancer = yes predicted as no
true_negative = 9560  # cancer = no predicted correctly

# Total samples
total_samples = true_positive + false_positive + false_negative + true_negative

# Metrics calculation
accuracy = (true_positive + true_negative) / total_samples
error_rate = (false_positive + false_negative) / total_samples
precision = true_positive / (true_positive + false_positive)
recall = true_positive / (true_positive + false_negative)

# Print the results
print(f"Accuracy: {accuracy:.4f}")
print(f"Error Rate: {error_rate:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")






Accuracy: The proportion of correctly classified samples. It's calculated as:

Accuracy = (TP + TN) / Total
Error Rate: The proportion of misclassified samples. It's calculated as:

Error Rate = (FP + FN) / Total
Precision: The proportion of positive predictions that are actually correct. It's calculated as:

Precision = TP / (TP + FP)
Recall: The proportion of actual positive cases that are correctly identified. It's calculated as:

Recall = TP / (TP + FN)
Expected Results

Based on the given confusion matrix:

True Positives (TP) = 90
False Positives (FP) = 140
False Negatives (FN) = 210
True Negatives (TN) = 9560
Total = 10,000 samples
Calculations:

Accuracy: (90 + 9560) / 10000 = 0.965
Error Rate: (140 + 210) / 10000 = 0.035
Precision: 90 / (90 + 140) ≈ 0.391
Recall: 90 / (90 + 210) ≈ 0.300

